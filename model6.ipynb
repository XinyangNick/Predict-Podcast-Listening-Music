{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a639f746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(749999, 11)\n",
      "Index(['Podcast_Name', 'Episode_Title', 'Episode_Length_minutes', 'Genre',\n",
      "       'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time',\n",
      "       'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment',\n",
      "       'Listening_Time_minutes'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Podcast_Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Episode_Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Episode_Length_minutes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Genre",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Host_Popularity_percentage",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publication_Day",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Publication_Time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Guest_Popularity_percentage",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number_of_Ads",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Episode_Sentiment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Listening_Time_minutes",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "4e7d7189-7fbe-44e6-986a-bf6e18c80ac0",
       "rows": [
        [
         "0",
         "Mystery Matters",
         "Episode 98",
         null,
         "True Crime",
         "74.81",
         "Thursday",
         "Night",
         null,
         "0.0",
         "Positive",
         "31.41998"
        ],
        [
         "1",
         "Joke Junction",
         "Episode 26",
         "119.8",
         "Comedy",
         "66.95",
         "Saturday",
         "Afternoon",
         "75.95",
         "2.0",
         "Negative",
         "88.01241"
        ],
        [
         "2",
         "Study Sessions",
         "Episode 16",
         "73.9",
         "Education",
         "69.97",
         "Tuesday",
         "Evening",
         "8.97",
         "0.0",
         "Negative",
         "44.92531"
        ],
        [
         "3",
         "Digital Digest",
         "Episode 45",
         "67.17",
         "Technology",
         "57.22",
         "Monday",
         "Morning",
         "78.7",
         "2.0",
         "Positive",
         "46.27824"
        ],
        [
         "4",
         "Mind & Body",
         "Episode 86",
         "110.51",
         "Health",
         "80.07",
         "Monday",
         "Afternoon",
         "58.68",
         "3.0",
         "Neutral",
         "75.61031"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Podcast_Name</th>\n",
       "      <th>Episode_Title</th>\n",
       "      <th>Episode_Length_minutes</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Host_Popularity_percentage</th>\n",
       "      <th>Publication_Day</th>\n",
       "      <th>Publication_Time</th>\n",
       "      <th>Guest_Popularity_percentage</th>\n",
       "      <th>Number_of_Ads</th>\n",
       "      <th>Episode_Sentiment</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mystery Matters</td>\n",
       "      <td>Episode 98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True Crime</td>\n",
       "      <td>74.81</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Night</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>31.41998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joke Junction</td>\n",
       "      <td>Episode 26</td>\n",
       "      <td>119.80</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>66.95</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>75.95</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>88.01241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Study Sessions</td>\n",
       "      <td>Episode 16</td>\n",
       "      <td>73.90</td>\n",
       "      <td>Education</td>\n",
       "      <td>69.97</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>8.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>44.92531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Digital Digest</td>\n",
       "      <td>Episode 45</td>\n",
       "      <td>67.17</td>\n",
       "      <td>Technology</td>\n",
       "      <td>57.22</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>78.70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>46.27824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mind &amp; Body</td>\n",
       "      <td>Episode 86</td>\n",
       "      <td>110.51</td>\n",
       "      <td>Health</td>\n",
       "      <td>80.07</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>58.68</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>75.61031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Podcast_Name Episode_Title  Episode_Length_minutes       Genre  \\\n",
       "0  Mystery Matters    Episode 98                     NaN  True Crime   \n",
       "1    Joke Junction    Episode 26                  119.80      Comedy   \n",
       "2   Study Sessions    Episode 16                   73.90   Education   \n",
       "3   Digital Digest    Episode 45                   67.17  Technology   \n",
       "4      Mind & Body    Episode 86                  110.51      Health   \n",
       "\n",
       "   Host_Popularity_percentage Publication_Day Publication_Time  \\\n",
       "0                       74.81        Thursday            Night   \n",
       "1                       66.95        Saturday        Afternoon   \n",
       "2                       69.97         Tuesday          Evening   \n",
       "3                       57.22          Monday          Morning   \n",
       "4                       80.07          Monday        Afternoon   \n",
       "\n",
       "   Guest_Popularity_percentage  Number_of_Ads Episode_Sentiment  \\\n",
       "0                          NaN            0.0          Positive   \n",
       "1                        75.95            2.0          Negative   \n",
       "2                         8.97            0.0          Negative   \n",
       "3                        78.70            2.0          Positive   \n",
       "4                        58.68            3.0           Neutral   \n",
       "\n",
       "   Listening_Time_minutes  \n",
       "0                31.41998  \n",
       "1                88.01241  \n",
       "2                44.92531  \n",
       "3                46.27824  \n",
       "4                75.61031  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv').drop(columns=['id'])\n",
    "train_df = train_df.dropna(subset=['Number_of_Ads'])\n",
    "print(train_df.shape)\n",
    "print(train_df.columns)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61bb588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets randomly and stratified by the target variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sub_train, sub_test = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70825b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599999, 10) (599999,)\n",
      "(150000, 10) (150000,)\n"
     ]
    }
   ],
   "source": [
    "# get the target variable and features\n",
    "X_sub_train = sub_train.drop(columns=['Listening_Time_minutes'])\n",
    "y_sub_train = sub_train['Listening_Time_minutes']\n",
    "X_sub_test = sub_test.drop(columns=['Listening_Time_minutes'])\n",
    "y_sub_test = sub_test['Listening_Time_minutes']\n",
    "print(X_sub_train.shape, y_sub_train.shape)\n",
    "print(X_sub_test.shape, y_sub_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3efc87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Episode_Length_minutes',\n",
       " 'Host_Popularity_percentage',\n",
       " 'Guest_Popularity_percentage',\n",
       " 'Number_of_Ads']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_attributes = X_sub_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_attributes = X_sub_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "num_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e117903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GroupMedianImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Imputes missing values in col_to_impute by taking\n",
    "    the median of that column within groups defined by group_cols.\n",
    "    \"\"\"\n",
    "    def __init__(self, group_cols, col_to_impute):\n",
    "        self.group_cols = group_cols\n",
    "        self.col_to_impute = col_to_impute\n",
    "        self.group_medians_ = None\n",
    "        self.global_median_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Convert to DataFrame if needed\n",
    "        df = pd.DataFrame(X.copy())\n",
    "        \n",
    "        # Compute groupwise median\n",
    "        self.group_medians_ = (\n",
    "            df.groupby(self.group_cols)[self.col_to_impute]\n",
    "              .median()\n",
    "              .to_dict()\n",
    "        )\n",
    "        # Compute overall median as fallback\n",
    "        self.global_median_ = df[self.col_to_impute].median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        df = pd.DataFrame(X.copy())\n",
    "        \n",
    "        # We'll apply groupwise median for each row\n",
    "        def fill_with_group_median(row):\n",
    "            group_key = tuple(row[col] for col in self.group_cols)\n",
    "            if pd.isna(row[self.col_to_impute]):\n",
    "                # if group exists in dictionary, use that median\n",
    "                # otherwise, use global median\n",
    "                return self.group_medians_.get(group_key, self.global_median_)\n",
    "            else:\n",
    "                return row[self.col_to_impute]\n",
    "        \n",
    "        df[self.col_to_impute] = df.apply(fill_with_group_median, axis=1)\n",
    "        \n",
    "        # Fill any remaining missing with global median\n",
    "        df[self.col_to_impute].fillna(self.global_median_, inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06f9f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "col_names = num_attributes\n",
    "\n",
    "#find the column number of the column 'Episode_Length_minutes', 'Guest_Popularity_percentage', 'Host_Popularity_percentage'\n",
    "episode_length_index = col_names.index('Episode_Length_minutes')\n",
    "guest_popularity_index = col_names.index('Guest_Popularity_percentage')\n",
    "host_popularity_index = col_names.index('Host_Popularity_percentage')\n",
    "\n",
    "class NewAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Adds new attributes to the DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self, add_new_attributes=1):\n",
    "        self.add_new_attributes = add_new_attributes\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "       \n",
    "        ideal_listening_time_guest1 = X[:, episode_length_index] * X[:, host_popularity_index] * 0.01\n",
    "\n",
    "        ideal_listening_time_guest2 = X[:, episode_length_index] * X[:, guest_popularity_index] * 0.01\n",
    "\n",
    "        # Add the new attributes to the DataFrame\n",
    "        ideal_listening_time_guest3 = X[:, episode_length_index] * (X[:, host_popularity_index] * X[:, guest_popularity_index] * 0.0001)\n",
    "\n",
    "        return np.c_[X, ideal_listening_time_guest1, ideal_listening_time_guest2]\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf096d85",
   "metadata": {},
   "source": [
    "# Model 5 Use median of global data set to impute.\n",
    "\n",
    "Also add new attribute that popularity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c757f540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads']\n",
      "['Podcast_Name', 'Episode_Title', 'Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Create a pipeline with the custom and simple imputers\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Assume X is your feature matrix\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "\n",
    "\n",
    "group_imputer = GroupMedianImputer(\n",
    "    group_cols=['Podcast_Name', 'Episode_Title'], \n",
    "    col_to_impute='Episode_Length_minutes'\n",
    ")\n",
    "\n",
    "group_imputer2 = GroupMedianImputer(\n",
    "    group_cols=['Podcast_Name', 'Episode_Title'], \n",
    "    col_to_impute='Guest_Popularity_percentage'\n",
    ")\n",
    "\n",
    "simple_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "\n",
    "imputer_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('group_imputer', group_imputer),\n",
    "        ('group_imputer2', group_imputer2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "col_names = X_sub_train.columns.tolist()\n",
    "\n",
    "\n",
    "attr_adder = NewAttributesAdder(add_new_attributes=1)\n",
    "\n",
    "\n",
    "num_attributes = X_sub_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_attributes = X_sub_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(num_attributes)\n",
    "\n",
    "print(cat_attributes)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('minmax', MinMaxScaler()),\n",
    "    ('imputer', simple_imputer),\n",
    "    ('attr_adder', attr_adder)\n",
    "])\n",
    "\n",
    "num_pipeline2 = Pipeline([\n",
    "    ('minmax', RobustScaler()),\n",
    "    ('imputer', simple_imputer),\n",
    "    ('attr_adder', attr_adder)\n",
    "])\n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "\n",
    "combine_pipeline = ColumnTransformer([\n",
    "    ('scaler', num_pipeline, num_attributes),\n",
    "    ('cat', cat_pipeline, cat_attributes)\n",
    "])\n",
    "\n",
    "combine_pipeline2 = ColumnTransformer([\n",
    "    ('scaler', num_pipeline2, num_attributes),\n",
    "    ('cat', cat_pipeline, cat_attributes)\n",
    "])\n",
    "\n",
    "# Fit the imputer pipeline on the training data\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('imputer', imputer_pipeline),\n",
    "    ('combine', combine_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20f9ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_grouped_imputed = combine_pipeline2.fit_transform(X_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d27cd830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.07788642991936867\n",
      "  (0, 1)\t0.24707055597107963\n",
      "  (0, 2)\t0.6570718575274385\n",
      "  (0, 3)\t-0.5\n",
      "  (0, 4)\t0.00019243443542780948\n",
      "  (0, 5)\t0.0005117698118330023\n",
      "  (0, 42)\t1.0\n",
      "  (0, 149)\t1.0\n",
      "  (0, 163)\t1.0\n",
      "  (0, 165)\t1.0\n",
      "  (0, 174)\t1.0\n",
      "  (0, 175)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(train_df_grouped_imputed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66da87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    scores = np.sqrt(-scores)  # Convert to RMSE\n",
    "    #print the variable name\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f5f6b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [13.43301682 13.33173052 13.4043473  13.30091122 13.33735673]\n",
      "Mean: 13.361472516266968\n",
      "Standard deviation: 0.04917451003026039\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Evaluate the models using cross-validation\n",
    "\n",
    "lin_scores = cross_val_score(lin_reg, train_df_grouped_imputed, \n",
    "                             y_sub_train, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "    \n",
    "display_scores(lin_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27308a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [13.46065267 13.36973227 13.4378435  13.33513602 13.37757936]\n",
      "Mean: 13.396188762864629\n",
      "Standard deviation: 0.04618312783445473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1, random_state=42)\n",
    "\n",
    "lasso_scores = cross_val_score(lasso_reg, train_df_grouped_imputed, \n",
    "                               y_sub_train, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "display_scores(lasso_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00c21ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [13.43322703 13.33259974 13.40440834 13.30128437 13.33733981]\n",
      "Mean: 13.361771859824206\n",
      "Standard deviation: 0.04905177921692511\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=0.1, random_state=42)\n",
    "ridge_scores = cross_val_score(ridge_reg, train_df_grouped_imputed, \n",
    "                                 y_sub_train, scoring='neg_mean_squared_error', cv=5)\n",
    "display_scores(ridge_scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85d7298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [13.43846967 13.3382359  13.41050901 13.30642665 13.34237906]\n",
      "Mean: 13.367204057662061\n",
      "Standard deviation: 0.04920047768678487\n"
     ]
    }
   ],
   "source": [
    "# # Add grid search for hyperparameter tuning\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = [\n",
    "#     {'alpha': [0.1, 1, 10]}\n",
    "# ]\n",
    "# ridge_reg = Ridge(random_state=42)\n",
    "# grid_search = GridSearchCV(ridge_reg, param_grid, cv=5,\n",
    "#                              scoring='neg_mean_squared_error')\n",
    "# grid_search.fit(train_df_grouped_imputed, y_sub_train)\n",
    "\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=10, random_state=42)\n",
    "ridge_scores = cross_val_score(ridge_reg, train_df_grouped_imputed, \n",
    "                                 y_sub_train, scoring='neg_mean_squared_error', cv=5)\n",
    "display_scores(ridge_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "793af3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [13.10816152 13.12456785 13.06170055]\n",
      "Mean: 13.098143309939745\n",
      "Standard deviation: 0.026625148016640385\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_reg = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_scores = cross_val_score(xgb_reg, train_df_grouped_imputed, \n",
    "                               y_sub_train, scoring='neg_mean_squared_error', cv=3)\n",
    "display_scores(xgb_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f071ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [13.02028735 13.0430344  12.97324611]\n",
      "Mean: 13.012189285551178\n",
      "Standard deviation: 0.029060684204455452\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_reg = XGBRegressor(n_estimators=300, learning_rate=0.1, max_depth=7, random_state=42)\n",
    "xgb_scores = cross_val_score(xgb_reg, train_df_grouped_imputed, \n",
    "                               y_sub_train, scoring='neg_mean_squared_error', cv=3)\n",
    "display_scores(xgb_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60cb8158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [12.94723877 12.95584414 12.88265409]\n",
      "Mean: 12.928579000272224\n",
      "Standard deviation: 0.03266329340100232\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_reg = XGBRegressor(n_estimators=500, learning_rate=0.08, max_depth=11, random_state=42)\n",
    "xgb_scores = cross_val_score(xgb_reg, train_df_grouped_imputed, \n",
    "                               y_sub_train, scoring='neg_mean_squared_error', cv=3)\n",
    "display_scores(xgb_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b632d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [12.8985731  12.90896444 12.83945082]\n",
      "Mean: 12.882329454228673\n",
      "Standard deviation: 0.030615113578508605\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_reg = XGBRegressor(n_estimators=500, learning_rate=0.04, max_depth=15, random_state=42)\n",
    "xgb_scores = cross_val_score(xgb_reg, train_df_grouped_imputed, \n",
    "                               y_sub_train, scoring='neg_mean_squared_error', cv=3)\n",
    "display_scores(xgb_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e441bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.04, 'max_depth': 15, 'n_estimators': 500}\n",
      "Best score: -165.955349452467\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [500],\n",
    "    'max_depth': [15, 19],\n",
    "    'learning_rate': [0.01, 0.04]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBRegressor(random_state=42), \n",
    "                           param_grid, \n",
    "                           cv=3,\n",
    "                           scoring='neg_mean_squared_error')  # or another metric relevant to your problem\n",
    "grid_search.fit(train_df_grouped_imputed, y_sub_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df45fc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6edcd196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [13.07659115 13.09490806 13.02825443]\n",
      "Mean: 13.066584547161204\n",
      "Standard deviation: 0.02811613663105719\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "\n",
    "estimators = [\n",
    "    ('LR', LinearRegression()),\n",
    "    ('xgb', XGBRegressor(random_state=42, n_estimators=100)),\n",
    "]\n",
    "\n",
    "# Replace LogisticRegression with a regressor, e.g., LinearRegression\n",
    "stacking_model = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
    "scores = cross_val_score(stacking_model, train_df_grouped_imputed,\n",
    "                                 y_sub_train, scoring='neg_mean_squared_error', cv=3)\n",
    "\n",
    "display_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e98d181b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [13.06126285 13.08255711 13.01853024]\n",
      "Mean: 13.054116729334504\n",
      "Standard deviation: 0.026622800446422352\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "estimators = [\n",
    "    ('LR', LinearRegression()),\n",
    "    ('xgb', XGBRegressor(random_state=42, n_estimators=100)),\n",
    "    ('DecisionTree', DecisionTreeRegressor(min_samples_split=1000, random_state=42)),\n",
    "]\n",
    "\n",
    "# Replace LogisticRegression with a regressor, e.g., LinearRegression\n",
    "stacking_model = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
    "scores = cross_val_score(stacking_model, train_df_grouped_imputed,\n",
    "                                 y_sub_train, scoring='neg_mean_squared_error', cv=3)\n",
    "\n",
    "display_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e21ba502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [13.07673201 13.09486604 13.02856462]\n",
      "Mean: 13.066720890268227\n",
      "Standard deviation: 0.02797780947269261\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "\n",
    "estimators = [\n",
    "    ('LR', LinearRegression()),\n",
    "    ('xgb', XGBRegressor(random_state=42, n_estimators=100)),\n",
    "    ('ridge', Ridge(alpha=0.1, random_state=42)),\n",
    "    ('lasso', Lasso(alpha=0.1, random_state=42)),\n",
    "]\n",
    "\n",
    "# Replace LogisticRegression with a regressor, e.g., LinearRegression\n",
    "stacking_model = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
    "scores = cross_val_score(stacking_model, train_df_grouped_imputed,\n",
    "                                 y_sub_train, scoring='neg_mean_squared_error', cv=3)\n",
    "\n",
    "display_scores(scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
