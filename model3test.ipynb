{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a639f746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(749999, 11)\n",
      "Index(['Podcast_Name', 'Episode_Title', 'Episode_Length_minutes', 'Genre',\n",
      "       'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time',\n",
      "       'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment',\n",
      "       'Listening_Time_minutes'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Podcast_Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Episode_Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Episode_Length_minutes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Genre",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Host_Popularity_percentage",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publication_Day",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Publication_Time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Guest_Popularity_percentage",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number_of_Ads",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Episode_Sentiment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Listening_Time_minutes",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c5f4284c-caf7-4d72-9a36-74f158eb3d48",
       "rows": [
        [
         "0",
         "Mystery Matters",
         "Episode 98",
         null,
         "True Crime",
         "74.81",
         "Thursday",
         "Night",
         null,
         "0.0",
         "Positive",
         "31.41998"
        ],
        [
         "1",
         "Joke Junction",
         "Episode 26",
         "119.8",
         "Comedy",
         "66.95",
         "Saturday",
         "Afternoon",
         "75.95",
         "2.0",
         "Negative",
         "88.01241"
        ],
        [
         "2",
         "Study Sessions",
         "Episode 16",
         "73.9",
         "Education",
         "69.97",
         "Tuesday",
         "Evening",
         "8.97",
         "0.0",
         "Negative",
         "44.92531"
        ],
        [
         "3",
         "Digital Digest",
         "Episode 45",
         "67.17",
         "Technology",
         "57.22",
         "Monday",
         "Morning",
         "78.7",
         "2.0",
         "Positive",
         "46.27824"
        ],
        [
         "4",
         "Mind & Body",
         "Episode 86",
         "110.51",
         "Health",
         "80.07",
         "Monday",
         "Afternoon",
         "58.68",
         "3.0",
         "Neutral",
         "75.61031"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Podcast_Name</th>\n",
       "      <th>Episode_Title</th>\n",
       "      <th>Episode_Length_minutes</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Host_Popularity_percentage</th>\n",
       "      <th>Publication_Day</th>\n",
       "      <th>Publication_Time</th>\n",
       "      <th>Guest_Popularity_percentage</th>\n",
       "      <th>Number_of_Ads</th>\n",
       "      <th>Episode_Sentiment</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mystery Matters</td>\n",
       "      <td>Episode 98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True Crime</td>\n",
       "      <td>74.81</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Night</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>31.41998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joke Junction</td>\n",
       "      <td>Episode 26</td>\n",
       "      <td>119.80</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>66.95</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>75.95</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>88.01241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Study Sessions</td>\n",
       "      <td>Episode 16</td>\n",
       "      <td>73.90</td>\n",
       "      <td>Education</td>\n",
       "      <td>69.97</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>8.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>44.92531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Digital Digest</td>\n",
       "      <td>Episode 45</td>\n",
       "      <td>67.17</td>\n",
       "      <td>Technology</td>\n",
       "      <td>57.22</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>78.70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>46.27824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mind &amp; Body</td>\n",
       "      <td>Episode 86</td>\n",
       "      <td>110.51</td>\n",
       "      <td>Health</td>\n",
       "      <td>80.07</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>58.68</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>75.61031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Podcast_Name Episode_Title  Episode_Length_minutes       Genre  \\\n",
       "0  Mystery Matters    Episode 98                     NaN  True Crime   \n",
       "1    Joke Junction    Episode 26                  119.80      Comedy   \n",
       "2   Study Sessions    Episode 16                   73.90   Education   \n",
       "3   Digital Digest    Episode 45                   67.17  Technology   \n",
       "4      Mind & Body    Episode 86                  110.51      Health   \n",
       "\n",
       "   Host_Popularity_percentage Publication_Day Publication_Time  \\\n",
       "0                       74.81        Thursday            Night   \n",
       "1                       66.95        Saturday        Afternoon   \n",
       "2                       69.97         Tuesday          Evening   \n",
       "3                       57.22          Monday          Morning   \n",
       "4                       80.07          Monday        Afternoon   \n",
       "\n",
       "   Guest_Popularity_percentage  Number_of_Ads Episode_Sentiment  \\\n",
       "0                          NaN            0.0          Positive   \n",
       "1                        75.95            2.0          Negative   \n",
       "2                         8.97            0.0          Negative   \n",
       "3                        78.70            2.0          Positive   \n",
       "4                        58.68            3.0           Neutral   \n",
       "\n",
       "   Listening_Time_minutes  \n",
       "0                31.41998  \n",
       "1                88.01241  \n",
       "2                44.92531  \n",
       "3                46.27824  \n",
       "4                75.61031  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv').drop(columns=['id'])\n",
    "train_df = train_df.dropna(subset=['Number_of_Ads'])\n",
    "print(train_df.shape)\n",
    "print(train_df.columns)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61bb588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets randomly and stratified by the target variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sub_train, sub_test = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70825b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599999, 10) (599999,)\n",
      "(150000, 10) (150000,)\n"
     ]
    }
   ],
   "source": [
    "# get the target variable and features\n",
    "X_sub_train = sub_train.drop(columns=['Listening_Time_minutes'])\n",
    "y_sub_train = sub_train['Listening_Time_minutes']\n",
    "X_sub_test = sub_test.drop(columns=['Listening_Time_minutes'])\n",
    "y_sub_test = sub_test['Listening_Time_minutes']\n",
    "print(X_sub_train.shape, y_sub_train.shape)\n",
    "print(X_sub_test.shape, y_sub_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e117903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GroupMedianImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Imputes missing values in col_to_impute by taking\n",
    "    the median of that column within groups defined by group_cols.\n",
    "    \"\"\"\n",
    "    def __init__(self, group_cols, col_to_impute):\n",
    "        self.group_cols = group_cols\n",
    "        self.col_to_impute = col_to_impute\n",
    "        self.group_medians_ = None\n",
    "        self.global_median_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Convert to DataFrame if needed\n",
    "        df = pd.DataFrame(X.copy())\n",
    "        \n",
    "        # Compute groupwise median\n",
    "        self.group_medians_ = (\n",
    "            df.groupby(self.group_cols)[self.col_to_impute]\n",
    "              .median()\n",
    "              .to_dict()\n",
    "        )\n",
    "        # Compute overall median as fallback\n",
    "        self.global_median_ = df[self.col_to_impute].median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        df = pd.DataFrame(X.copy())\n",
    "        \n",
    "        # We'll apply groupwise median for each row\n",
    "        def fill_with_group_median(row):\n",
    "            group_key = tuple(row[col] for col in self.group_cols)\n",
    "            if pd.isna(row[self.col_to_impute]):\n",
    "                # if group exists in dictionary, use that median\n",
    "                # otherwise, use global median\n",
    "                return self.group_medians_.get(group_key, self.global_median_)\n",
    "            else:\n",
    "                return row[self.col_to_impute]\n",
    "        \n",
    "        df[self.col_to_impute] = df.apply(fill_with_group_median, axis=1)\n",
    "        \n",
    "        # Fill any remaining missing with global median\n",
    "        df[self.col_to_impute].fillna(self.global_median_, inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "class MissingValueIndcator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Adds a binary indicator for missing values in specified columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Add a dummy column 'missing_<col>' for each column in self.cols\n",
    "        df = pd.DataFrame(X.copy())\n",
    "        for col in self.cols:\n",
    "            df[f'missing_{col}'] = df[col].isna().astype(int)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864de9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf096d85",
   "metadata": {},
   "source": [
    "# Model 2 Use median of Podcast name and episode title to predict add a dummy variable indicate whether value is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c757f540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads']\n",
      "['Podcast_Name', 'Episode_Title', 'Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Create a pipeline with the custom and simple imputers\n",
    "\n",
    "group_imputer = GroupMedianImputer(\n",
    "    group_cols=['Podcast_Name', 'Episode_Title'], \n",
    "    col_to_impute='Episode_Length_minutes'\n",
    ")\n",
    "\n",
    "group_imputer2 = GroupMedianImputer(\n",
    "    group_cols=['Podcast_Name', 'Episode_Title'], \n",
    "    col_to_impute='Guest_Popularity_percentage'\n",
    ")\n",
    "\n",
    "missing_value_indicator = MissingValueIndcator(\n",
    "    cols=['Episode_Length_minutes', 'Guest_Popularity_percentage']\n",
    ")\n",
    "\n",
    "simple_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "imputer_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('group_imputer', group_imputer),\n",
    "        ('group_imputer2', group_imputer2)\n",
    "    ]\n",
    ")\n",
    "num_attributes = X_sub_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_attributes = X_sub_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(num_attributes)\n",
    "\n",
    "print(cat_attributes)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('minmax', MinMaxScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "combine_pipeline = ColumnTransformer([\n",
    "    ('scaler', num_pipeline, num_attributes),\n",
    "    ('cat', cat_pipeline, cat_attributes)\n",
    "])\n",
    "\n",
    "\n",
    "# Fit the imputer pipeline on the training data\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('missing_value_indicator', missing_value_indicator),\n",
    "    ('imputer', imputer_pipeline),\n",
    "    ('combine', combine_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20f9ad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14335\\AppData\\Local\\Temp\\ipykernel_30280\\2962990983.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[self.col_to_impute].fillna(self.global_median_, inplace=True)\n",
      "C:\\Users\\14335\\AppData\\Local\\Temp\\ipykernel_30280\\2962990983.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[self.col_to_impute].fillna(self.global_median_, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "train_df_grouped_imputed = full_pipeline.fit_transform(X_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d27cd830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.5611997660623276\n",
      "  (0, 1)\t0.580991875423155\n",
      "  (0, 2)\t0.7112834625969477\n",
      "  (0, 40)\t1.0\n",
      "  (0, 147)\t1.0\n",
      "  (0, 161)\t1.0\n",
      "  (0, 163)\t1.0\n",
      "  (0, 172)\t1.0\n",
      "  (0, 173)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(train_df_grouped_imputed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f3985f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce dimensionality\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "X_sub_train_pca = pca.fit_transform(train_df_grouped_imputed.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f5f6b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [13.651131   13.57403178 13.60718125 13.50663172 13.56989717]\n",
      "Mean: 13.581774582721277\n",
      "Standard deviation: 0.047546369130095446\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Evaluate the models using cross-validation\n",
    "\n",
    "lin_scores = cross_val_score(lin_reg, X_sub_train_pca, \n",
    "                             y_sub_train, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "\n",
    "def display_scores(scores):\n",
    "    scores = np.sqrt(-scores)  # Convert to RMSE\n",
    "    #print the variable name\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "    \n",
    "display_scores(lin_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e21ba502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [14.17242386 14.09494837 14.14241181 14.03929165 14.08862937]\n",
      "Mean: 14.107541009546347\n",
      "Standard deviation: 0.0460433633931517\n"
     ]
    }
   ],
   "source": [
    "dec_reg = DecisionTreeRegressor(max_depth=3, random_state=42, min_samples_leaf=1000)\n",
    "\n",
    "dec_scores = cross_val_score(dec_reg, X_sub_train_pca,\n",
    "                             y_sub_train, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "display_scores(dec_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
