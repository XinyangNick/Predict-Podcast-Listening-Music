{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a639f746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(749999, 11)\n",
      "Index(['Podcast_Name', 'Episode_Title', 'Episode_Length_minutes', 'Genre',\n",
      "       'Host_Popularity_percentage', 'Publication_Day', 'Publication_Time',\n",
      "       'Guest_Popularity_percentage', 'Number_of_Ads', 'Episode_Sentiment',\n",
      "       'Listening_Time_minutes'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Podcast_Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Episode_Title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Episode_Length_minutes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Genre",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Host_Popularity_percentage",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Publication_Day",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Publication_Time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Guest_Popularity_percentage",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Number_of_Ads",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Episode_Sentiment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Listening_Time_minutes",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e110adaa-5372-4b81-94dc-12074fc0bd97",
       "rows": [
        [
         "0",
         "Mystery Matters",
         "Episode 98",
         null,
         "True Crime",
         "74.81",
         "Thursday",
         "Night",
         null,
         "0.0",
         "Positive",
         "31.41998"
        ],
        [
         "1",
         "Joke Junction",
         "Episode 26",
         "119.8",
         "Comedy",
         "66.95",
         "Saturday",
         "Afternoon",
         "75.95",
         "2.0",
         "Negative",
         "88.01241"
        ],
        [
         "2",
         "Study Sessions",
         "Episode 16",
         "73.9",
         "Education",
         "69.97",
         "Tuesday",
         "Evening",
         "8.97",
         "0.0",
         "Negative",
         "44.92531"
        ],
        [
         "3",
         "Digital Digest",
         "Episode 45",
         "67.17",
         "Technology",
         "57.22",
         "Monday",
         "Morning",
         "78.7",
         "2.0",
         "Positive",
         "46.27824"
        ],
        [
         "4",
         "Mind & Body",
         "Episode 86",
         "110.51",
         "Health",
         "80.07",
         "Monday",
         "Afternoon",
         "58.68",
         "3.0",
         "Neutral",
         "75.61031"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Podcast_Name</th>\n",
       "      <th>Episode_Title</th>\n",
       "      <th>Episode_Length_minutes</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Host_Popularity_percentage</th>\n",
       "      <th>Publication_Day</th>\n",
       "      <th>Publication_Time</th>\n",
       "      <th>Guest_Popularity_percentage</th>\n",
       "      <th>Number_of_Ads</th>\n",
       "      <th>Episode_Sentiment</th>\n",
       "      <th>Listening_Time_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mystery Matters</td>\n",
       "      <td>Episode 98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True Crime</td>\n",
       "      <td>74.81</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Night</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>31.41998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joke Junction</td>\n",
       "      <td>Episode 26</td>\n",
       "      <td>119.80</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>66.95</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>75.95</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>88.01241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Study Sessions</td>\n",
       "      <td>Episode 16</td>\n",
       "      <td>73.90</td>\n",
       "      <td>Education</td>\n",
       "      <td>69.97</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Evening</td>\n",
       "      <td>8.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>44.92531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Digital Digest</td>\n",
       "      <td>Episode 45</td>\n",
       "      <td>67.17</td>\n",
       "      <td>Technology</td>\n",
       "      <td>57.22</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Morning</td>\n",
       "      <td>78.70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>46.27824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mind &amp; Body</td>\n",
       "      <td>Episode 86</td>\n",
       "      <td>110.51</td>\n",
       "      <td>Health</td>\n",
       "      <td>80.07</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>58.68</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>75.61031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Podcast_Name Episode_Title  Episode_Length_minutes       Genre  \\\n",
       "0  Mystery Matters    Episode 98                     NaN  True Crime   \n",
       "1    Joke Junction    Episode 26                  119.80      Comedy   \n",
       "2   Study Sessions    Episode 16                   73.90   Education   \n",
       "3   Digital Digest    Episode 45                   67.17  Technology   \n",
       "4      Mind & Body    Episode 86                  110.51      Health   \n",
       "\n",
       "   Host_Popularity_percentage Publication_Day Publication_Time  \\\n",
       "0                       74.81        Thursday            Night   \n",
       "1                       66.95        Saturday        Afternoon   \n",
       "2                       69.97         Tuesday          Evening   \n",
       "3                       57.22          Monday          Morning   \n",
       "4                       80.07          Monday        Afternoon   \n",
       "\n",
       "   Guest_Popularity_percentage  Number_of_Ads Episode_Sentiment  \\\n",
       "0                          NaN            0.0          Positive   \n",
       "1                        75.95            2.0          Negative   \n",
       "2                         8.97            0.0          Negative   \n",
       "3                        78.70            2.0          Positive   \n",
       "4                        58.68            3.0           Neutral   \n",
       "\n",
       "   Listening_Time_minutes  \n",
       "0                31.41998  \n",
       "1                88.01241  \n",
       "2                44.92531  \n",
       "3                46.27824  \n",
       "4                75.61031  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv').drop(columns=['id'])\n",
    "train_df = train_df.dropna(subset=['Number_of_Ads'])\n",
    "print(train_df.shape)\n",
    "print(train_df.columns)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61bb588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets randomly and stratified by the target variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sub_train, sub_test = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70825b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599999, 10) (599999,)\n",
      "(150000, 10) (150000,)\n"
     ]
    }
   ],
   "source": [
    "# get the target variable and features\n",
    "X_sub_train = sub_train.drop(columns=['Listening_Time_minutes'])\n",
    "y_sub_train = sub_train['Listening_Time_minutes']\n",
    "X_sub_test = sub_test.drop(columns=['Listening_Time_minutes'])\n",
    "y_sub_test = sub_test['Listening_Time_minutes']\n",
    "print(X_sub_train.shape, y_sub_train.shape)\n",
    "print(X_sub_test.shape, y_sub_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3efc87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Episode_Length_minutes',\n",
       " 'Host_Popularity_percentage',\n",
       " 'Guest_Popularity_percentage',\n",
       " 'Number_of_Ads']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_attributes = X_sub_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_attributes = X_sub_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "num_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e117903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GroupMedianImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Imputes missing values in col_to_impute by taking\n",
    "    the median of that column within groups defined by group_cols.\n",
    "    \"\"\"\n",
    "    def __init__(self, group_cols, col_to_impute):\n",
    "        self.group_cols = group_cols\n",
    "        self.col_to_impute = col_to_impute\n",
    "        self.group_medians_ = None\n",
    "        self.global_median_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Convert to DataFrame if needed\n",
    "        df = pd.DataFrame(X.copy())\n",
    "        \n",
    "        # Compute groupwise median\n",
    "        self.group_medians_ = (\n",
    "            df.groupby(self.group_cols)[self.col_to_impute]\n",
    "              .median()\n",
    "              .to_dict()\n",
    "        )\n",
    "        # Compute overall median as fallback\n",
    "        self.global_median_ = df[self.col_to_impute].median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        df = pd.DataFrame(X.copy())\n",
    "        \n",
    "        # We'll apply groupwise median for each row\n",
    "        def fill_with_group_median(row):\n",
    "            group_key = tuple(row[col] for col in self.group_cols)\n",
    "            if pd.isna(row[self.col_to_impute]):\n",
    "                # if group exists in dictionary, use that median\n",
    "                # otherwise, use global median\n",
    "                return self.group_medians_.get(group_key, self.global_median_)\n",
    "            else:\n",
    "                return row[self.col_to_impute]\n",
    "        \n",
    "        df[self.col_to_impute] = df.apply(fill_with_group_median, axis=1)\n",
    "        \n",
    "        # Fill any remaining missing with global median\n",
    "        df[self.col_to_impute].fillna(self.global_median_, inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06f9f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "col_names = num_attributes\n",
    "\n",
    "#find the column number of the column 'Episode_Length_minutes', 'Guest_Popularity_percentage', 'Host_Popularity_percentage'\n",
    "episode_length_index = col_names.index('Episode_Length_minutes')\n",
    "guest_popularity_index = col_names.index('Guest_Popularity_percentage')\n",
    "host_popularity_index = col_names.index('Host_Popularity_percentage')\n",
    "\n",
    "class NewAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Adds new attributes to the DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self, add_new_attributes=1):\n",
    "        self.add_new_attributes = add_new_attributes\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "       \n",
    "        ideal_listening_time_guest1 = X[:, episode_length_index] * X[:, host_popularity_index] * 0.01\n",
    "\n",
    "        ideal_listening_time_guest2 = X[:, episode_length_index] * X[:, guest_popularity_index] * 0.01\n",
    "\n",
    "        # Add the new attributes to the DataFrame\n",
    "        ideal_listening_time_guest3 = X[:, episode_length_index] * (X[:, host_popularity_index] * X[:, guest_popularity_index] * 0.0001)\n",
    "\n",
    "        return np.c_[X, ideal_listening_time_guest1, ideal_listening_time_guest2]\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf096d85",
   "metadata": {},
   "source": [
    "# Model 7 Use median of global data set to impute.\n",
    "\n",
    "Also add new attribute that popularity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c757f540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Episode_Length_minutes', 'Host_Popularity_percentage', 'Guest_Popularity_percentage', 'Number_of_Ads']\n",
      "['Podcast_Name', 'Episode_Title', 'Genre', 'Publication_Day', 'Publication_Time', 'Episode_Sentiment']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Create a pipeline with the custom and simple imputers\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Assume X is your feature matrix\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "\n",
    "\n",
    "group_imputer = GroupMedianImputer(\n",
    "    group_cols=['Podcast_Name', 'Episode_Title'], \n",
    "    col_to_impute='Episode_Length_minutes'\n",
    ")\n",
    "\n",
    "group_imputer2 = GroupMedianImputer(\n",
    "    group_cols=['Podcast_Name', 'Episode_Title'], \n",
    "    col_to_impute='Guest_Popularity_percentage'\n",
    ")\n",
    "\n",
    "simple_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "\n",
    "imputer_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('group_imputer', group_imputer),\n",
    "        ('group_imputer2', group_imputer2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "col_names = X_sub_train.columns.tolist()\n",
    "\n",
    "\n",
    "attr_adder = NewAttributesAdder(add_new_attributes=1)\n",
    "\n",
    "\n",
    "num_attributes = X_sub_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_attributes = X_sub_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(num_attributes)\n",
    "\n",
    "print(cat_attributes)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('minmax', MinMaxScaler()),\n",
    "    ('imputer', simple_imputer),\n",
    "    ('attr_adder', attr_adder)\n",
    "])\n",
    "\n",
    "num_pipeline2 = Pipeline([\n",
    "    ('minmax', RobustScaler()),\n",
    "    ('imputer', simple_imputer),\n",
    "    ('attr_adder', attr_adder)\n",
    "])\n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "\n",
    "combine_pipeline = ColumnTransformer([\n",
    "    ('scaler', num_pipeline, num_attributes),\n",
    "    ('cat', cat_pipeline, cat_attributes)\n",
    "])\n",
    "\n",
    "combine_pipeline2 = ColumnTransformer([\n",
    "    ('scaler', num_pipeline2, num_attributes),\n",
    "    ('cat', cat_pipeline, cat_attributes)\n",
    "])\n",
    "\n",
    "# Fit the imputer pipeline on the training data\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('imputer', imputer_pipeline),\n",
    "    ('combine', combine_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20f9ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_grouped_imputed = combine_pipeline2.fit_transform(X_sub_train)\n",
    "train_df_grouped_imputed_test = combine_pipeline2.transform(X_sub_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d27cd830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.07788642991936867\n",
      "  (0, 1)\t0.24707055597107963\n",
      "  (0, 2)\t0.6570718575274385\n",
      "  (0, 3)\t-0.5\n",
      "  (0, 4)\t0.00019243443542780948\n",
      "  (0, 5)\t0.0005117698118330023\n",
      "  (0, 42)\t1.0\n",
      "  (0, 149)\t1.0\n",
      "  (0, 163)\t1.0\n",
      "  (0, 165)\t1.0\n",
      "  (0, 174)\t1.0\n",
      "  (0, 175)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(train_df_grouped_imputed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66da87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    scores = np.sqrt(-scores)  # Convert to RMSE\n",
    "    #print the variable name\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b632d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# xgb_reg = XGBRegressor(n_estimators=500, learning_rate=0.04, max_depth=15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aa5c4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=2, n_estimators=450; total time=  31.2s\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=2, n_estimators=450; total time=  29.1s\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=2, n_estimators=450; total time=  29.1s\n",
      "[CV] END learning_rate=0.03, max_depth=14, min_child_weight=1, n_estimators=450; total time=  34.6s\n",
      "[CV] END learning_rate=0.03, max_depth=14, min_child_weight=1, n_estimators=450; total time=  38.1s\n",
      "[CV] END learning_rate=0.03, max_depth=14, min_child_weight=1, n_estimators=450; total time=  34.0s\n",
      "[CV] END learning_rate=0.03, max_depth=16, min_child_weight=2, n_estimators=500; total time=  52.1s\n",
      "[CV] END learning_rate=0.03, max_depth=16, min_child_weight=2, n_estimators=500; total time=  50.7s\n",
      "[CV] END learning_rate=0.03, max_depth=16, min_child_weight=2, n_estimators=500; total time=  50.6s\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=2, n_estimators=500; total time=  33.1s\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=2, n_estimators=500; total time=  31.1s\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=2, n_estimators=500; total time=  31.0s\n",
      "[CV] END learning_rate=0.03, max_depth=16, min_child_weight=1, n_estimators=450; total time=  52.6s\n",
      "[CV] END learning_rate=0.03, max_depth=16, min_child_weight=1, n_estimators=450; total time=  53.0s\n",
      "[CV] END learning_rate=0.03, max_depth=16, min_child_weight=1, n_estimators=450; total time=  53.4s\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=1, n_estimators=500; total time=  32.8s\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=1, n_estimators=500; total time=  33.2s\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=1, n_estimators=500; total time=  33.4s\n",
      "[CV] END learning_rate=0.03, max_depth=15, min_child_weight=1, n_estimators=500; total time=  45.0s\n",
      "[CV] END learning_rate=0.03, max_depth=15, min_child_weight=1, n_estimators=500; total time=  45.0s\n",
      "[CV] END learning_rate=0.03, max_depth=15, min_child_weight=1, n_estimators=500; total time=  45.3s\n",
      "[CV] END learning_rate=0.05, max_depth=15, min_child_weight=3, n_estimators=500; total time=  33.8s\n",
      "[CV] END learning_rate=0.05, max_depth=15, min_child_weight=3, n_estimators=500; total time=  33.5s\n",
      "[CV] END learning_rate=0.05, max_depth=15, min_child_weight=3, n_estimators=500; total time=  33.9s\n",
      "[CV] END learning_rate=0.03, max_depth=14, min_child_weight=2, n_estimators=500; total time=  33.3s\n",
      "[CV] END learning_rate=0.03, max_depth=14, min_child_weight=2, n_estimators=500; total time=  33.7s\n",
      "[CV] END learning_rate=0.03, max_depth=14, min_child_weight=2, n_estimators=500; total time=  33.7s\n",
      "[CV] END learning_rate=0.03, max_depth=15, min_child_weight=2, n_estimators=450; total time=  39.2s\n",
      "[CV] END learning_rate=0.03, max_depth=15, min_child_weight=2, n_estimators=450; total time=  38.3s\n",
      "[CV] END learning_rate=0.03, max_depth=15, min_child_weight=2, n_estimators=450; total time=  39.3s\n",
      "[CV] END learning_rate=0.04, max_depth=16, min_child_weight=2, n_estimators=500; total time=  46.5s\n",
      "[CV] END learning_rate=0.04, max_depth=16, min_child_weight=2, n_estimators=500; total time=  46.1s\n",
      "[CV] END learning_rate=0.04, max_depth=16, min_child_weight=2, n_estimators=500; total time=  46.7s\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=3, n_estimators=450; total time=  26.6s\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=3, n_estimators=450; total time=  26.8s\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=3, n_estimators=450; total time=  27.2s\n",
      "[CV] END learning_rate=0.05, max_depth=15, min_child_weight=2, n_estimators=500; total time=  35.9s\n",
      "[CV] END learning_rate=0.05, max_depth=15, min_child_weight=2, n_estimators=500; total time=  35.9s\n",
      "[CV] END learning_rate=0.05, max_depth=15, min_child_weight=2, n_estimators=500; total time=  36.4s\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=3, n_estimators=550; total time=  30.8s\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=3, n_estimators=550; total time=  30.8s\n",
      "[CV] END learning_rate=0.04, max_depth=14, min_child_weight=3, n_estimators=550; total time=  31.2s\n",
      "[CV] END learning_rate=0.05, max_depth=15, min_child_weight=2, n_estimators=550; total time=  38.7s\n",
      "[CV] END learning_rate=0.05, max_depth=15, min_child_weight=2, n_estimators=550; total time=  35.7s\n",
      "[CV] END learning_rate=0.05, max_depth=15, min_child_weight=2, n_estimators=550; total time=  36.2s\n",
      "[CV] END learning_rate=0.04, max_depth=16, min_child_weight=1, n_estimators=450; total time=  47.6s\n",
      "[CV] END learning_rate=0.04, max_depth=16, min_child_weight=1, n_estimators=450; total time=  46.9s\n",
      "[CV] END learning_rate=0.04, max_depth=16, min_child_weight=1, n_estimators=450; total time=  46.8s\n",
      "[CV] END learning_rate=0.05, max_depth=16, min_child_weight=1, n_estimators=500; total time=  50.3s\n",
      "[CV] END learning_rate=0.05, max_depth=16, min_child_weight=1, n_estimators=500; total time=  44.8s\n",
      "[CV] END learning_rate=0.05, max_depth=16, min_child_weight=1, n_estimators=500; total time=  44.9s\n",
      "[CV] END learning_rate=0.05, max_depth=14, min_child_weight=3, n_estimators=500; total time=  23.4s\n",
      "[CV] END learning_rate=0.05, max_depth=14, min_child_weight=3, n_estimators=500; total time=  24.2s\n",
      "[CV] END learning_rate=0.05, max_depth=14, min_child_weight=3, n_estimators=500; total time=  26.8s\n",
      "[CV] END learning_rate=0.05, max_depth=14, min_child_weight=1, n_estimators=500; total time=  29.8s\n",
      "[CV] END learning_rate=0.05, max_depth=14, min_child_weight=1, n_estimators=500; total time=  30.3s\n",
      "[CV] END learning_rate=0.05, max_depth=14, min_child_weight=1, n_estimators=500; total time=  31.1s\n",
      "[CV] END learning_rate=0.04, max_depth=15, min_child_weight=2, n_estimators=500; total time=  36.6s\n",
      "[CV] END learning_rate=0.04, max_depth=15, min_child_weight=2, n_estimators=500; total time=  35.8s\n",
      "[CV] END learning_rate=0.04, max_depth=15, min_child_weight=2, n_estimators=500; total time=  35.2s\n",
      "Best parameters: {'n_estimators': 500, 'min_child_weight': 2, 'max_depth': 16, 'learning_rate': 0.03}\n",
      "Best RMSE: 12.873655400753439\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [450, 500, 550],\n",
    "    'learning_rate': [0.03, 0.04, 0.05],\n",
    "    'max_depth': [14, 15, 16],\n",
    "    'min_child_weight': [1, 2, 3],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,  # Try 20 random combinations (tweak as needed)\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(train_df_grouped_imputed, y_sub_train)\n",
    "\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "# print rmse\n",
    "print(\"Best RMSE:\", np.sqrt(-random_search.best_score_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23a4d4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 12.684557049978524\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "xgb_reg = XGBRegressor(**random_search.best_params_)\n",
    "xgb_reg.fit(train_df_grouped_imputed, y_sub_train)\n",
    "y_pred = xgb_reg.predict(train_df_grouped_imputed_test)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_sub_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "print(\"Test RMSE:\", rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
